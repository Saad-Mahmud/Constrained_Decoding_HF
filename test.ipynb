{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93941572-285c-4e1c-88a3-c2a40b75bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import models, gen, select,capture\n",
    "import sys,os\n",
    "import contextlib\n",
    "from IPython.utils.io import capture_output\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "befd275d-538b-43c1-94ae-56a70ba8eedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbr-saad/anaconda3/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da001352-3df8-4d3b-a5b8-97051df2c6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/rbr-saad/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token=\"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\")\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e682cde6-f511-49b2-8bbe-ed23e8dbea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c0c40b66-1d42-4dac-88fe-a1e7dbd7b095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.13it/s]\n",
      "/home/rbr-saad/anaconda3/envs/nlp/lib/python3.12/site-packages/guidance/chat.py:73: UserWarning: Chat template {%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content'] %}\n",
      "    {%- set loop_messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set loop_messages = messages %}\n",
      "{%- endif %}\n",
      "\n",
      "{{- bos_token }}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n",
      "        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n",
      "    {%- endif %}\n",
      "    {%- if message['role'] == 'user' %}\n",
      "        {%- if loop.last and system_message is defined %}\n",
      "            {{- '[INST] ' + system_message + '\\n\\n' + message['content'] + '[/INST]' }}\n",
      "        {%- else %}\n",
      "            {{- '[INST] ' + message['content'] + '[/INST]' }}\n",
      "        {%- endif %}\n",
      "    {%- elif message['role'] == 'assistant' %}\n",
      "        {{- ' ' + message['content'] + eos_token}}\n",
      "    {%- else %}\n",
      "        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      " was unable to be loaded directly into guidance.\n",
      "                        Defaulting to the ChatML format which may not be optimal for the selected model. \n",
      "                        For best results, create and pass in a `guidance.ChatTemplate` subclass for your model.\n",
      "  warnings.warn(f\"\"\"Chat template {chat_template} was unable to be loaded directly into guidance.\n"
     ]
    }
   ],
   "source": [
    "model = models.Transformers(model_name, quantization_config = nf4_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a536c870-7a92-4813-b4d7-df9c028b7119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Pick a random number from 1 to 5: <span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>3</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = model+\"Pick a random number from 1 to 5: \"+ capture(select(['1','2','3','4','5']), \"ans\")\n",
    "f[\"ans\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dbcd2637-cd27-4d37-b8b1-b378e28d02cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_gen(t):\n",
    "    with capture_output() as captured:\n",
    "        a = model+\"Pick a random number from 1 to 5: \"+ gen(regex = '(1|2|3|4|5)$', temperature = t, name = 'text')\n",
    "        return int(a['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b6322383-e866-4991-b750-53fc9c286c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_gen(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "573d2b88-42b8-47be-b0be-9563d3012603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "/tmp/ipykernel_2686565/3236110381.py:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  prompt = prompt+f'\\{ {keys[0]}: '\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "f-string: expecting '}', or format specs (3236110381.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[138], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    prompt = prompt+f'\\{ {keys[0]}: '\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: expecting '}', or format specs\n"
     ]
    }
   ],
   "source": [
    "def dict2string(prompt, keys, rangs):\n",
    "    prompt = prompt+f'{ {keys[0]}: '\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3fe80374-b666-4bc9-ab5b-6f35f666c17d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to set.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdict2string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgen a dict of 2 number: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFirst Number\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSecond Number\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[134], line 2\u001b[0m, in \u001b[0;36mdict2string\u001b[0;34m(prompt, keys, rangs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdict2string\u001b[39m(prompt, keys, rangs):\n\u001b[0;32m----> 2\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m 1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to set.__format__"
     ]
    }
   ],
   "source": [
    "dict2string(\"gen a dict of 2 number: \", [\"First Number\", \"Second Number\"], [()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53c706ce-71e4-4e84-b989-8f7923719dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate the dictionary with keys and ranges\n",
    "def generate_floating_point_dict(lm, prompt, key_ranges, max_tokens = 5,  temp=0.8):\n",
    "    key_gens = []\n",
    "    for key, (low, high) in key_ranges.items():\n",
    "        if low < 0 and high < 0:\n",
    "            regex = f\"(-[0-9]+(\\\\.[0-9]+)?)\"\n",
    "        elif low < 0 and high >= 0:\n",
    "            regex = f\"(-?[0-9]+(\\\\.[0-9]+)?)\"\n",
    "        else:\n",
    "            regex = f\"([0-9]+(\\\\.[0-9]+)?)\"\n",
    "        key_gens.append(f'\"{key}\": {gen(key, regex=regex, max_tokens=max_tokens, temperature = temp)}, ')\n",
    "    key_gens[-1] =key_gens[-1][:-2] \n",
    "    keys_string = \"\".join(key_gens)\n",
    "    \n",
    "    lm += prompt+f\"\"\"\\\n",
    "    {{{keys_string}}}\"\"\"\n",
    "    return lm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f5967f4-661b-4310-8c73-72ce97df7aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize model\n",
    "model = models.Transformers(model_name, quantization_config = nf4_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7416692d-f349-40d1-be76-b339cd61df69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Generate 3 random floating point numbers between 100.0 to 200.0:     {&quot;1st number&quot;:<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>1</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>2</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>5</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>.</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>3</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>,</span> &quot;2nd number&quot;:<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>1</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>7</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>8</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>.</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>9</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>,</span> &quot;3rd number&quot;:<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>1</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>9</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>7</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>.</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>2</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>}</span></pre>"
      ],
      "text/plain": [
       "<guidance.models.transformers._transformers.Transformers at 0x6ffdee12aa20>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "key_ranges = {\n",
    "    '1st number': (100.0, 200.0),\n",
    "    '2nd number': (100.0, 200.0),\n",
    "    '3rd number': (100.0, 200.0)\n",
    "}\n",
    "prompt= \"Generate 3 random floating point numbers between 100.0 to 200.0: \"\n",
    "# Generate the dictionary\n",
    "generate_floating_point_dict(model, prompt, key_ranges,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617e8fa-6d4c-4643-bf2e-9c6c80e97bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
